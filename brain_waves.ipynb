{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has 9366 rows and 18 columns\n",
      "The test data has 4801 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print('The train data has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "print('The test data has {} rows and {} columns'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  portfolio_id      desk_id    office_id pf_category  start_date       sold  \\\n",
      "0   PF00001002  DSK00001001  OFF00001002           B    20040720  110000000   \n",
      "1   PF00001003  DSK00001002  OFF00001001           A    20040709  176671000   \n",
      "2   PF00001005  DSK00001004  OFF00001001           A    20040723   56474000   \n",
      "3   PF00001006  DSK00001005  OFF00001001           A    20040609  164813000   \n",
      "4   PF00001007  DSK00001005  OFF00001002           B    20040609  140800000   \n",
      "\n",
      "  country_code  euribor_rate currency  libor_rate        bought  \\\n",
      "0            T       0.02074      USD    2.332216  1.098097e+08   \n",
      "1            N       0.02074      GBP    5.269617  1.760084e+08   \n",
      "2            T       0.02074      USD    2.332216  5.637953e+07   \n",
      "3            T       0.02074      USD    2.332216  1.645088e+08   \n",
      "4            T       0.02074      USD    2.332216  1.405402e+08   \n",
      "\n",
      "   creation_date indicator_code  sell_date type hedge_value status   return  \n",
      "0       20040720            NaN   20040812    B         NaN    NaN  0.02496  \n",
      "1       20040723            NaN   20040812    C         NaN    NaN  0.05496  \n",
      "2       20040723            NaN   20040817    A         NaN    NaN  0.02496  \n",
      "3       20040723            NaN   20040713    A         NaN    NaN  0.02496  \n",
      "4       20040723            NaN   20040713    B         NaN    NaN  0.02496  \n"
     ]
    }
   ],
   "source": [
    "print(train.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['return']\n",
    "train=train.drop('return',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop('portfolio_id',axis=1)\n",
    "test=test.drop('portfolio_id',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([train, test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "desk_id           5613\n",
       "office_id            0\n",
       "pf_category          0\n",
       "start_date           0\n",
       "sold                 2\n",
       "country_code         0\n",
       "euribor_rate         0\n",
       "currency             0\n",
       "libor_rate         739\n",
       "bought               2\n",
       "creation_date        0\n",
       "indicator_code    8550\n",
       "sell_date            0\n",
       "type                 0\n",
       "hedge_value       8552\n",
       "status            4541\n",
       "dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14167 entries, 0 to 4800\n",
      "Data columns (total 16 columns):\n",
      "desk_id           8554 non-null object\n",
      "office_id         14167 non-null object\n",
      "pf_category       14167 non-null object\n",
      "start_date        14167 non-null int64\n",
      "sold              14165 non-null float64\n",
      "country_code      14167 non-null object\n",
      "euribor_rate      14167 non-null float64\n",
      "currency          14167 non-null object\n",
      "libor_rate        13428 non-null float64\n",
      "bought            14165 non-null float64\n",
      "creation_date     14167 non-null int64\n",
      "indicator_code    5617 non-null object\n",
      "sell_date         14167 non-null int64\n",
      "type              14167 non-null object\n",
      "hedge_value       5615 non-null object\n",
      "status            9626 non-null object\n",
      "dtypes: float64(4), int64(3), object(9)\n",
      "memory usage: 1.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['libor_rate'].fillna(df_all['libor_rate'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "desk_id           5613\n",
       "office_id            0\n",
       "pf_category          0\n",
       "start_date           0\n",
       "sold                 2\n",
       "country_code         0\n",
       "euribor_rate         0\n",
       "currency             0\n",
       "libor_rate           0\n",
       "bought               2\n",
       "creation_date        0\n",
       "indicator_code    8550\n",
       "sell_date            0\n",
       "type                 0\n",
       "hedge_value       8552\n",
       "status            4541\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['sold'].fillna(df_all['sold'].median(), inplace=True)\n",
    "df_all['bought'].fillna(df_all['bought'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import Imputer\n",
    "#imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "#df_all=imp.fit_transform(df_all) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "#X = pd.DataFrame(data)\n",
    "xt = DataFrameImputer().fit_transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all['status'] = df_all['status'].fillna(df_all['status'].value_counts().index[0])c\n",
    "#df_all['hedge_value'] = df_all['status'].fillna(df_all['status'].value_counts().index[0])\n",
    "df_all = df_all.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "#df_all['hedge_value'].fillna(False, inplace=True)\n",
    "df_all['status'].fillna(False, inplace=True)\n",
    "#test['hedge_value'].fillna(False, inplace=True)\n",
    "#df_all['bought'].fillna(df_all['bought'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "desk_id           0\n",
       "office_id         0\n",
       "pf_category       0\n",
       "start_date        0\n",
       "sold              0\n",
       "country_code      0\n",
       "euribor_rate      0\n",
       "currency          0\n",
       "libor_rate        0\n",
       "bought            0\n",
       "creation_date     0\n",
       "indicator_code    0\n",
       "sell_date         0\n",
       "type              0\n",
       "hedge_value       0\n",
       "status            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all=df_all.drop('desk_id',axis=1)\n",
    "df_all=df_all.drop('indicator_code',axis=1)\n",
    "df_all=df_all.drop('hedge_value',axis=1)\n",
    "df_all=df_all.drop('status',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=df_all.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['desk_id', 'office_id', 'pf_category', 'country_code', 'currency',\n",
      "       'type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in cf:\n",
    "    df_all[i]=le.fit_transform(df_all[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_all['desk_id']=le.fit_transform(df_all['desk_id'])\n",
    "#df_all['indicator_code']=le.fit_transform(df_all['indicator_code'])\n",
    "#df_all['status']=le.fit_transform(df_all['status'])\n",
    "#print(le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "##scaler = MinMaxScaler()\n",
    "#df_all=scaler.fit_transform(df_all)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_all=scaler.fit_transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0         1         2         3         4         5         6   \\\n",
      "0 -2.705573  0.672186 -0.437003 -2.532929  0.044765  0.919245  0.883079   \n",
      "1 -2.704858 -1.487683 -1.514454 -2.533292  0.424661  0.066512  0.883079   \n",
      "2 -2.703429 -1.487683 -1.514454 -2.532830 -0.260230  0.919245  0.883079   \n",
      "3 -2.702715 -1.487683 -1.514454 -2.536589  0.357094  0.919245  0.883079   \n",
      "4 -2.702715  0.672186 -0.437003 -2.536589  0.220266  0.919245  0.883079   \n",
      "\n",
      "         7         8         9         10  11        12        13        14  \\\n",
      "0  1.403640  0.985558  0.044158 -2.533295   0 -2.542583 -0.958504 -0.393318   \n",
      "1  0.300368  3.083167  0.421505 -2.533196   0 -2.542583 -0.391170 -0.393318   \n",
      "2  1.403640  0.985558 -0.260404 -2.533196   0 -2.542418 -1.525837 -0.393318   \n",
      "3  1.403640  0.985558  0.355955 -2.533196   0 -2.545836 -1.525837 -0.393318   \n",
      "4  1.403640  0.985558  0.219328 -2.533196   0 -2.545836 -0.958504 -0.393318   \n",
      "\n",
      "   15  \n",
      "0   0  \n",
      "1   0  \n",
      "2   0  \n",
      "3   0  \n",
      "4   0  \n"
     ]
    }
   ],
   "source": [
    "df_all=pd.DataFrame(df_all)\n",
    "print(df_all.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=len(train)\n",
    "X_train = df_all[:num_train]\n",
    "X_test = df_all[num_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_tr1,X_te1,y_tr1,y_te1=train_test_split(X_train,target,test_size=0.1,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-maccuracy:-526.161\teval-maccuracy:-585.8\n",
      "[20]\ttrain-maccuracy:-155.764\teval-maccuracy:-173.446\n",
      "[40]\ttrain-maccuracy:-45.7565\teval-maccuracy:-50.9016\n",
      "[60]\ttrain-maccuracy:-13.0763\teval-maccuracy:-14.4874\n",
      "[80]\ttrain-maccuracy:-3.34637\teval-maccuracy:-3.65771\n",
      "[100]\ttrain-maccuracy:-0.443285\teval-maccuracy:-0.435279\n",
      "[120]\ttrain-maccuracy:0.434707\teval-maccuracy:0.533375\n",
      "[140]\ttrain-maccuracy:0.70944\teval-maccuracy:0.828735\n",
      "[160]\ttrain-maccuracy:0.801173\teval-maccuracy:0.920653\n",
      "[180]\ttrain-maccuracy:0.834542\teval-maccuracy:0.952082\n",
      "[200]\ttrain-maccuracy:0.850973\teval-maccuracy:0.96325\n",
      "[220]\ttrain-maccuracy:0.860071\teval-maccuracy:0.968077\n",
      "[240]\ttrain-maccuracy:0.865654\teval-maccuracy:0.970897\n",
      "[260]\ttrain-maccuracy:0.869422\teval-maccuracy:0.97293\n",
      "[280]\ttrain-maccuracy:0.875614\teval-maccuracy:0.974219\n",
      "[300]\ttrain-maccuracy:0.879582\teval-maccuracy:0.975771\n",
      "[320]\ttrain-maccuracy:0.884026\teval-maccuracy:0.976667\n",
      "r2 score train and evaluation is 0.8887475748368505,0.9777654971764626\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "def multAcc(pred, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    acc = r2_score(label, pred)\n",
    "    return 'maccuracy', acc\n",
    "\n",
    "import xgboost as xgb\n",
    "params = {\n",
    "    'n_trees': 1500, \n",
    "    'eta': 0.03,\n",
    "    'max_depth':45,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight' :7,\n",
    "    'lambda':9,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'silent': 1\n",
    "}\n",
    "#dtrain = xgb.DMatrix(data=X_tr1, label=y_tr1)\n",
    "dvalid = xgb.DMatrix(data=X_te1, label=y_te1)\n",
    "#dtr = xgb.DMatrix(X_tr1, y_tr1)\n",
    "#dte = xgb.DMatrix(X_te1)\n",
    "#watchlist = [(dtr, 'train'),(dte, 'eval')]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=target)\n",
    "#dvalid = xgb.DMatrix(data=X_train, label=target)\n",
    "#dtrain = xgb.DMatrix(data=X_tr1, label=y_tr1)\n",
    "#dvalid = xgb.DMatrix(data=X_te1, label=y_te1)\n",
    "dtest = xgb.DMatrix(data=X_test)\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_boost_rounds = 1200\n",
    "#model = xgb.train(xgb_params, dtrain, num_boost_round=num_boost_rounds,verbose_eval=20)\n",
    "#model=xgb.train(params, dtrain, 1700, watchlist,maximize=True, verbose_eval=20,feval=multAcc)\n",
    "\n",
    "model=xgb.train(params, dtrain, 340, watchlist, maximize=True, verbose_eval=20, feval=multAcc)\n",
    "\n",
    "\n",
    "xgb_pred = model.predict(dtrain)\n",
    "xgb_pred1 = model.predict(dvalid)\n",
    "r2=r2_score(target,xgb_pred)\n",
    "#r2=r2_score(y_tr1,xgb_pred)\n",
    "r2_1=r2_score(y_te1,xgb_pred1)\n",
    "#mse1=mean_squared_error(y_te1,pred1)\n",
    "\n",
    "print(\"r2 score train and evaluation is {},{}\".format(r2,r2_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train-maccuracy:0.945442\teval-maccuracy:0.810505\n",
    "train-maccuracy:0.92745\teval-maccuracy:0.815367\n",
    "train-maccuracy:0.929279\teval-maccuracy:0.815984  #15,0.01\n",
    "train-maccuracy:0.917333\teval-maccuracy:0.856178 #15,0.01,child_weight:2\n",
    "train-maccuracy:0.900859\teval-maccuracy:0.875528  #15,0.01,child_weight:3\n",
    "train-maccuracy:0.885925\teval-maccuracy:0.888673  #15,0.01,child_weight:4\n",
    "train-maccuracy:0.972998\teval-maccuracy:0.961316\n",
    "train-maccuracy:0.992797\teval-maccuracy:0.955204\n",
    "r2 score train and evaluation is 0.9229597807749278,0.9460040433767868 with removing all\n",
    "r2 score train and evaluation is 0.9892839631659844,0.9430579961462168\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred=pd.read_csv('xgb_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      portfolio_id    return\n",
      "0       PF00001001  0.024934\n",
      "1       PF00001004  0.025117\n",
      "2       PF00001009  0.024552\n",
      "3       PF00001013  0.025226\n",
      "4       PF00001014  0.024862\n"
     ]
    }
   ],
   "source": [
    "print(xgb_pred.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score train and evaluation of light gbm is 0.9068864266576513,0.9467903954905736\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=188,\n",
    "                              learning_rate=0.3, n_estimators=9720,\n",
    "                              max_bin = 55, bagging_fraction = 1,max_depth=90,                    \n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, num_iterations=2500,min_sum_hessian_in_leaf = 11)\n",
    "model_lgb.fit(X_tr1,y_tr1)\n",
    "lgb_pred = model_lgb.predict(X_tr1)\n",
    "lgb_pred1 = model_lgb.predict(X_te1)\n",
    "\n",
    "r2=r2_score(y_tr1,lgb_pred)\n",
    "\n",
    "r2_1=r2_score(y_te1,lgb_pred1)\n",
    "\n",
    "print(\"r2 score train and evaluation of light gbm is {},{}\".format(r2,r2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_pred = model.predict(dtest)\n",
    "lgb_predi=model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble_train = xgb_pred*0.7 + lgb_pred*0.3#+stack_pred*0.6\n",
    "#ensemble_valuation = xgb_pred1*0.7+ lgb_pred1*0.3#+stack_pred1*0.6\n",
    "#r2=r2_score(y_tr1,ensemble_train)\n",
    "\n",
    "#r2_1=r2_score(y_te1,ensemble_valuation)\n",
    "#print(\"result of ensemble is {},{}\".format(r2,r2_1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgb_pred['return'] *0.13 + stack_pred*0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9366, 13)\n",
      "(8429, 13)\n",
      "(8429, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_tr1.shape)\n",
    "print(y_tr1.shape)\n",
    "#assert(y_tr1.shape==(-1,1))\n",
    "#y_tr1=y_tr1.reshape((-1,1))\n",
    "#print(y_tr1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_tr1\n",
    "y=y_tr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_tr1)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_tr1, y_tr1, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-10404c46343f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n\u001b[1;32m      3\u001b[0m                                                  meta_model = lasso)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstacked_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#score = rmsle_cv(stacked_averaged_models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-840f39156110>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_models_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mout_of_fold_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    705\u001b[0m                              \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                              \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                              multi_output=True, y_numeric=True)\n\u001b[0m\u001b[1;32m    708\u001b[0m             y = check_array(y, order='F', copy=False, dtype=X.dtype.type,\n\u001b[1;32m    709\u001b[0m                             ensure_2d=False)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[0;32m--> 576\u001b[0;31m                         dtype=None)\n\u001b[0m\u001b[1;32m    577\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#X_tr1=X_tr1.reshape(X_tr1.shape[0],X_tr1.shape[1]).astype('float64')\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "stacked_averaged_models.fit(X_tr1, y_tr1)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "#score = rmsle_cv(stacked_averaged_models)\n",
    "#print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build and fit a model..\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pred_union', FeatureUnion(n_jobs=2,\n",
      "       transformer_list=[('ridge', Pipeline(memory=None,\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('poly_feats', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('ridge', RidgeTransformer(...=None)), ('lin_regr', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:520: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self.fit(X, y, **fit_params).transform(X)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-1d97af5bcd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#print(X_tr1.type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#print(y_tr1.type)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mstack_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mstack_pred1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                        **fit_params):\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mXs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "class RidgeTransformer(Ridge, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class RandomForestTransformer(RandomForestRegressor, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return self.predict(X)\n",
    "\n",
    "\n",
    "class KNeighborsTransformer(KNeighborsRegressor, TransformerMixin):\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        return self.predict(X)\n",
    "    \n",
    "def build_model():\n",
    "    ridge_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('poly_feats', PolynomialFeatures()),\n",
    "        ('ridge', RidgeTransformer())\n",
    "    ])\n",
    "\n",
    "    pred_union = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('ridge', ridge_transformer),\n",
    "            ('rand_forest', RandomForestTransformer()),\n",
    "            ('knn', KNeighborsTransformer())\n",
    "        ],\n",
    "        n_jobs=2\n",
    "    )\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('pred_union', pred_union),\n",
    "        ('lin_regr', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "print('Build and fit a model..')\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "#X_train1,target1 = make_regression()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(model)\n",
    "#X_tr1, X_te1, y_tr1, y_te1 = train_test_split(X_train,target, test_size=0.1)\n",
    "X_tr1=pd.DataFrame(X_tr1)\n",
    "y_tr1=pd.DataFrame(y_tr1)\n",
    "#print(X_tr1.type)\n",
    "#print(y_tr1.type)\n",
    "model.fit(X_tr1,y_tr1)\n",
    "stack_pred = model.predict(X_tr1)\n",
    "stack_pred1 = model.predict(X_te1)\n",
    "\n",
    "r2=r2_score(y_tr1,stack_pred)\n",
    "\n",
    "r2_1=r2_score(y_te1,stack_pred1)\n",
    "\n",
    "print(\"r2 score train and evaluation of light gbm is {},{}\".format(r2,r2_1))\n",
    "#score = model.score(X_test, y_test)\n",
    "\n",
    "#print('Done. Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# Initializing models\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "#from sklearn.linear_model import \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.linear_model import ARDRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import r2_score\n",
    "lr = LinearRegression()\n",
    "svr_lin = SVR(kernel='linear')\n",
    "ridge = Ridge(random_state=1)\n",
    "svr= SVR()\n",
    "Elasticnet=ElasticNet()\n",
    "extra_tree=ExtraTreesRegressor()\n",
    "lasso=Lasso()\n",
    "ada_boost=AdaBoostRegressor()\n",
    "Bayesianridge=BayesianRidge()\n",
    "random=RandomForestRegressor()\n",
    "knn=KNeighborsRegressor()\n",
    "gradient_boost=GradientBoostingRegressor()\n",
    "xgb = XGBRegressor() \n",
    "huber=HuberRegressor()\n",
    "sgd=SGDRegressor()\n",
    "tree=DecisionTreeRegressor()\n",
    "naive_bayes=GaussianNB()\n",
    "BR=BaggingRegressor()\n",
    "#ARD=ARDRegression()\n",
    "stregr = StackingRegressor(regressors=[lasso,svr,ridge,BR,random,Elasticnet,Bayesianridge,gradient_boost,knn,ada_boost,\n",
    "                                      extra_tree, huber,sgd,tree], \n",
    "                           meta_regressor= xgb)\n",
    "\n",
    "#params = {'lasso__alpha': [ 1.0, 2.0],\n",
    "         # 'ridge__alpha': [ 1.0, 2.0]\n",
    "         # }\n",
    "\n",
    "#grid = GridSearchCV(estimator=stregr, \n",
    "                   # param_grid=params, \n",
    "                    #cv=5,\n",
    "                    #refit=True)\n",
    "#grid.fit(X_tr1, y_tr1)\n",
    "\n",
    "#for params, mean_score, scores in grid.grid_scores_:\n",
    "        #print(\"%0.3f +/- %0.2f %r\"\n",
    "              #% (mean_score, scores.std() / 2.0, params))\n",
    "\n",
    "\n",
    "\n",
    "#pred= grid.best_estimator_.predict(X_tr1)\n",
    "#pred1=grid.best_estimator_.predict(X_te1)\n",
    "\n",
    "#stregr = StackingRegressor(regressors=[ lasso,ridge,Elasticnet,Bayesianridge,gradient_boost,knn], \n",
    "                           #meta_regressor= random_forest)\n",
    "#print(grid.best_estimator_)\n",
    "# Training the stacking classifier\n",
    "\n",
    "stregr.fit(X_train, target)\n",
    "#stregr.fit(X_tr1, y_tr1)\n",
    "#stack_pred=stregr.predict(X_tr1)\n",
    "#stack_pred1=stregr.predict(X_te1)\n",
    "# Evaluate and visualize the fit\n",
    "#r2=r2_score(y_tr1,pred)\n",
    "#r2=r2_score(y_tr1,stack_pred)\n",
    "#r2_1=r2_score(y_te1,stack_pred1)\n",
    "print('training complete')\n",
    "#print(\"r2 score train and evaluation is {},{}\".format(r2,r2_1))#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r2 score train and evaluation is 0.9832486449878456,0.9721228426904391\n",
    "r2 score train and evaluation is 0.9997665315042173,0.9729329262688503\n",
    "r2 score train and evaluation is 0.9997705850245412,0.9723789307656796\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 0.0148 (0.0112)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet score: 0.0146 (0.0114)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Ridge score: 0.0124 (0.0160)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting score: 0.0100 (0.0140)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgboost score: 0.0139 (0.0129)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 0.0123 (0.0132)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-393-a50979b56256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                  meta_model = lasso)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmsle_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_averaged_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Stacking Averaged models score: {:.4f} ({:.4f})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-379-ad3c94629ded>\u001b[0m in \u001b[0;36mrmsle_cv\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrmsle_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mrmse\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_mean_squared_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlasso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRobustScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-392-dfca4af6e9d1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_models_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0mout_of_fold_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mholdout_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    705\u001b[0m                              \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                              \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                              multi_output=True, y_numeric=True)\n\u001b[0m\u001b[1;32m    708\u001b[0m             y = check_array(y, order='F', copy=False, dtype=X.dtype.type,\n\u001b[1;32m    709\u001b[0m                             ensure_2d=False)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n\u001b[0;32m--> 576\u001b[0;31m                         dtype=None)\n\u001b[0m\u001b[1;32m    577\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean square error and r2 score of xgb is 0.9066840209813242,3.588618098926056e-05\n",
    "r2 score and mean square error of xgb is 0.9072522212760351,3.566767030300321e-05\n",
    "0.96440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_pred = stregr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgb_pred['return'] *0.7 + stack_pred*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['return'] = pred\n",
    "#sub['return'] = sub['return'].astype(int)\n",
    "sub.to_csv('Jishnu.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data has 348978 rows and 51 columns\n",
      "The test data has 523466 rows and 50 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv('train_1.csv')\n",
    "test = pd.read_csv('test_1.csv')\n",
    "print('The train data has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n",
    "print('The test data has {} rows and {} columns'.format(test.shape[0], test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_id     num_var_1  num_var_2  num_var_3     num_var_4  \\\n",
      "0          id_11  2.302632e-08   0.040182          0  1.800000e-07   \n",
      "1          id_33  7.965789e-06   0.157872          0  2.105000e-06   \n",
      "2          id_51  7.828947e-08   0.089140          0  3.550000e-07   \n",
      "3          id_54  7.894737e-08   0.227239          0  1.050000e-06   \n",
      "4          id_62  3.321053e-06   0.160410          0  2.105000e-06   \n",
      "\n",
      "      num_var_5     num_var_6     num_var_7 cat_var_1 cat_var_2  ...    \\\n",
      "0  2.302632e-08  2.368421e-08  1.115205e-08       NaN        ce  ...     \n",
      "1  2.769737e-07  7.965789e-06  2.433058e-06        da        tn  ...     \n",
      "2  4.671053e-08  1.052632e-07  4.276014e-07        gf        ce  ...     \n",
      "3  1.381579e-07  2.190789e-07  1.848054e-08       NaN        ce  ...     \n",
      "4  2.769737e-07  3.340789e-06  2.152983e-06        da        tn  ...     \n",
      "\n",
      "  cat_var_34 cat_var_35 cat_var_36 cat_var_37 cat_var_38 cat_var_39  \\\n",
      "0          0          0          0          0          0          0   \n",
      "1          0          0          0          0          0          0   \n",
      "2          0          0          0          0          0          0   \n",
      "3          0          0          0          0          0          0   \n",
      "4          0          0          0          0          0          0   \n",
      "\n",
      "  cat_var_40 cat_var_41 cat_var_42 target  \n",
      "0          0          0          0      0  \n",
      "1          0          0          0      0  \n",
      "2          0          0          0      0  \n",
      "3          0          0          0      0  \n",
      "4          0          0          0      0  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train.head(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['target']\n",
    "train=train.drop('target',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop('transaction_id',axis=1)\n",
    "test=test.drop('transaction_id',axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([train, test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=pd.read_csv('new_df_all_with_four.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 872444 entries, 0 to 523465\n",
      "Data columns (total 49 columns):\n",
      "num_var_1     872444 non-null float64\n",
      "num_var_2     872444 non-null float64\n",
      "num_var_3     872444 non-null float64\n",
      "num_var_4     872444 non-null float64\n",
      "num_var_5     872444 non-null float64\n",
      "num_var_6     872444 non-null float64\n",
      "num_var_7     872444 non-null float64\n",
      "cat_var_1     837970 non-null object\n",
      "cat_var_2     872444 non-null object\n",
      "cat_var_3     775229 non-null object\n",
      "cat_var_4     872444 non-null object\n",
      "cat_var_5     872444 non-null object\n",
      "cat_var_6     850501 non-null object\n",
      "cat_var_7     872444 non-null object\n",
      "cat_var_8     754568 non-null object\n",
      "cat_var_9     872444 non-null object\n",
      "cat_var_10    872444 non-null object\n",
      "cat_var_11    872444 non-null object\n",
      "cat_var_12    872444 non-null object\n",
      "cat_var_13    872444 non-null object\n",
      "cat_var_14    872444 non-null object\n",
      "cat_var_15    872444 non-null object\n",
      "cat_var_16    872444 non-null object\n",
      "cat_var_17    872444 non-null object\n",
      "cat_var_18    872444 non-null object\n",
      "cat_var_19    872444 non-null int64\n",
      "cat_var_20    872444 non-null int64\n",
      "cat_var_21    872444 non-null int64\n",
      "cat_var_22    872444 non-null int64\n",
      "cat_var_23    872444 non-null int64\n",
      "cat_var_24    872444 non-null int64\n",
      "cat_var_25    872444 non-null int64\n",
      "cat_var_26    872444 non-null int64\n",
      "cat_var_27    872444 non-null int64\n",
      "cat_var_28    872444 non-null int64\n",
      "cat_var_29    872444 non-null int64\n",
      "cat_var_30    872444 non-null int64\n",
      "cat_var_31    872444 non-null int64\n",
      "cat_var_32    872444 non-null int64\n",
      "cat_var_33    872444 non-null int64\n",
      "cat_var_34    872444 non-null int64\n",
      "cat_var_35    872444 non-null int64\n",
      "cat_var_36    872444 non-null int64\n",
      "cat_var_37    872444 non-null int64\n",
      "cat_var_38    872444 non-null int64\n",
      "cat_var_39    872444 non-null int64\n",
      "cat_var_40    872444 non-null int64\n",
      "cat_var_41    872444 non-null int64\n",
      "cat_var_42    872444 non-null int64\n",
      "dtypes: float64(7), int64(24), object(18)\n",
      "memory usage: 332.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872444\n"
     ]
    }
   ],
   "source": [
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@@@0.num_var_1          0\n",
    "@@@1.num_var_2          0\n",
    "2.num_var_3          0\n",
    "3.num_var_4          0\n",
    "@@@4.num_var_5          0\n",
    "@@@5.num_var_6          0\n",
    "@@@@6.num_var_7          0\n",
    "@@@7.cat_var_1      34474\n",
    "@@@8.cat_var_2          0\n",
    "@@@9.cat_var_3      97215\n",
    "10.cat_var_4          0\n",
    "11.cat_var_5          0\n",
    "@@@12.cat_var_6      21943\n",
    "13.cat_var_7          0\n",
    "@@@14.cat_var_8     117876\n",
    "---@@@15.cat_var_9          0\n",
    "----@@@16.cat_var_10         0\n",
    "@@@17.cat_var_11         0\n",
    "@@@18,cat_var_12         0\n",
    "@@@19.cat_var_13         0\n",
    "@@@20.cat_var_14         0\n",
    "@@@21.cat_var_15         0\n",
    "22.cat_var_16         0\n",
    "@@@23.cat_var_17         0\n",
    "@@@24.cat_var_18         0\n",
    "25.cat_var_19         0\n",
    "26.cat_var_20         0\n",
    "27.cat_var_21         0\n",
    "@@@28.cat_var_22         0\n",
    "29.cat_var_23         0\n",
    "30.cat_var_24         0\n",
    "31.cat_var_25         0\n",
    "32.cat_var_26         0\n",
    "33.cat_var_27         0\n",
    "34.cat_var_28         0\n",
    "35.cat_var_29         0\n",
    "36.cat_var_30         0\n",
    "37.cat_var_31         0\n",
    "38.cat_var_32         0\n",
    "39.cat_var_33         0\n",
    "40.cat_var_34         0\n",
    "41.cat_var_35         0\n",
    "42.cat_var_36         0\n",
    "43.cat_var_37         0\n",
    "44.cat_var_38         0\n",
    "45.cat_var_39         0\n",
    "46.cat_var_40         0\n",
    "47.cat_var_41         0\n",
    "48.cat_var_42         0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_1=(df_all['num_var_7'],df_all['cat_var_3'],df_all['num_var_1'],df_all['num_var_6'],df_all['cat_var_15'],df_all['cat_var_17'],df_all['num_var_2'],df_all['cat_var_14'])\n",
    "df_all['num_var_7'],\n",
    "df_all['cat_var_3'],\n",
    "df_all['num_var_1'],\n",
    "df_all['num_var_6'],\n",
    "df_all['cat_var_15'],\n",
    "df_all['cat_var_17'],\n",
    "df_all['num_var_2'],\n",
    "df_all['cat_var_14']\n",
    "cat_var_1\n",
    "cat_var_8\n",
    "num_var_5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "PandasError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPandasError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-50827e026af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_all_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    289\u001b[0m                                          copy=False)\n\u001b[1;32m    290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mPandasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPandasError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "1. feature 6 (0.168841)\n",
    "2. feature 9 (0.164323)\n",
    "3. feature 0 (0.135010)\n",
    "4. feature 5 (0.095527)\n",
    "5. feature 21 (0.041294)\n",
    "6. feature 23 (0.037992)\n",
    "7. feature 1 (0.037941)\n",
    "8. feature 20 (0.031304)\n",
    "9. feature 7 (0.026507)\n",
    "10. feature 14 (0.026296)\n",
    "11. feature 4 (0.025587)\n",
    "---12. feature 12 (0.025329)\n",
    "---13. feature 16 (0.024375) not good\n",
    "---14. feature 15 (0.021755) not good\n",
    "----15. feature 22 (0.021519)not good\n",
    "-----16. feature 8 (0.019772) ok\n",
    "------17. feature 19 (0.017827) not good\n",
    "-------18. feature 18 (0.013268)not good\n",
    "--------19. feature 17 (0.013215)not good\n",
    "---------20. feature 24 (0.013082)not good\n",
    "21. feature 25 (0.009202) not good\n",
    "22. feature 11 (0.008804) not good\n",
    "23. feature 3 (0.006342)@ already there\n",
    "24. feature 30 (0.002714) not good\n",
    "25. feature 26 (0.002544)not good\n",
    "26. feature 13 (0.002433)not good\n",
    "27. feature 10 (0.002145)not good\n",
    "28. feature 27 (0.001949)not good\n",
    "29. feature 32 (0.001353)not good\n",
    "30. feature 35 (0.000528)not good\n",
    "31. feature 28 (0.000462)not good\n",
    "32. feature 29 (0.000387)not good\n",
    "33. feature 34 (0.000154)not good\n",
    "34. feature 31 (0.000149)not good\n",
    "35. feature 33 (0.000024)not good\n",
    "36. feature 2 (0.000022)already there\n",
    "37. feature 38 (0.000011)not good\n",
    "38. feature 37 (0.000008)not good\n",
    "39. feature 36 (0.000003)not good\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_var_1          0\n",
       "num_var_2          0\n",
       "num_var_3          0\n",
       "num_var_4          0\n",
       "num_var_5          0\n",
       "num_var_6          0\n",
       "num_var_7          0\n",
       "cat_var_1      34474\n",
       "cat_var_2          0\n",
       "cat_var_3      97215\n",
       "cat_var_4          0\n",
       "cat_var_5          0\n",
       "cat_var_6      21943\n",
       "cat_var_7          0\n",
       "cat_var_8     117876\n",
       "cat_var_9          0\n",
       "cat_var_10         0\n",
       "cat_var_11         0\n",
       "cat_var_12         0\n",
       "cat_var_13         0\n",
       "cat_var_14         0\n",
       "cat_var_15         0\n",
       "cat_var_16         0\n",
       "cat_var_17         0\n",
       "cat_var_18         0\n",
       "cat_var_19         0\n",
       "cat_var_20         0\n",
       "cat_var_21         0\n",
       "cat_var_22         0\n",
       "cat_var_23         0\n",
       "cat_var_24         0\n",
       "cat_var_25         0\n",
       "cat_var_26         0\n",
       "cat_var_27         0\n",
       "cat_var_28         0\n",
       "cat_var_29         0\n",
       "cat_var_30         0\n",
       "cat_var_31         0\n",
       "cat_var_32         0\n",
       "cat_var_33         0\n",
       "cat_var_34         0\n",
       "cat_var_35         0\n",
       "cat_var_36         0\n",
       "cat_var_37         0\n",
       "cat_var_38         0\n",
       "cat_var_39         0\n",
       "cat_var_40         0\n",
       "cat_var_41         0\n",
       "cat_var_42         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all=df_all.drop('num_var_1',axis=1)\n",
    "#df_all=df_all.drop('num_var_2',axis=1)\n",
    "df_all=df_all.drop('num_var_3',axis=1)\n",
    "df_all=df_all.drop('num_var_4',axis=1)\n",
    "#df_all=df_all.drop('num_var_5',axis=1)\n",
    "#df_all=df_all.drop('num_var_6',axis=1)\n",
    "#df_all=df_all.drop('num_var_7',axis=1)\n",
    "#df_all=df_all.drop('cat_var_1',axis=1)\n",
    "#df_all=df_all.drop('cat_var_2',axis=1)\n",
    "#df_all=df_all.drop('cat_var_3',axis=1)\n",
    "df_all=df_all.drop('cat_var_4',axis=1)\n",
    "df_all=df_all.drop('cat_var_5',axis=1)\n",
    "#df_all=df_all.drop('cat_var_6',axis=1)\n",
    "df_all=df_all.drop('cat_var_7',axis=1)\n",
    "#df_all=df_all.drop('cat_var_8',axis=1)\n",
    "df_all=df_all.drop('cat_var_9',axis=1)\n",
    "df_all=df_all.drop('cat_var_10',axis=1)\n",
    "#df_all=df_all.drop('cat_var_11',axis=1)\n",
    "#df_all=df_all.drop('cat_var_12',axis=1)\n",
    "#df_all=df_all.drop('cat_var_13',axis=1)\n",
    "#df_all=df_all.drop('cat_var_14',axis=1)\n",
    "#df_all=df_all.drop('cat_var_15',axis=1)\n",
    "df_all=df_all.drop('cat_var_16',axis=1)\n",
    "#df_all=df_all.drop('cat_var_17',axis=1)\n",
    "#df_all=df_all.drop('cat_var_18',axis=1)\n",
    "df_all=df_all.drop('cat_var_19',axis=1)\n",
    "df_all=df_all.drop('cat_var_20',axis=1)\n",
    "df_all=df_all.drop('cat_var_21',axis=1)\n",
    "#df_all=df_all.drop('cat_var_22',axis=1)\n",
    "df_all=df_all.drop('cat_var_23',axis=1)\n",
    "#df_all=df_all.drop('cat_var_24',axis=1)new one\n",
    "df_all=df_all.drop('cat_var_25',axis=1)\n",
    "df_all=df_all.drop('cat_var_26',axis=1)\n",
    "df_all=df_all.drop('cat_var_27',axis=1)\n",
    "df_all=df_all.drop('cat_var_28',axis=1)\n",
    "df_all=df_all.drop('cat_var_29',axis=1)\n",
    "df_all=df_all.drop('cat_var_30',axis=1)\n",
    "df_all=df_all.drop('cat_var_31',axis=1)\n",
    "df_all=df_all.drop('cat_var_32',axis=1)\n",
    "df_all=df_all.drop('cat_var_33',axis=1)\n",
    "df_all=df_all.drop('cat_var_34',axis=1)\n",
    "df_all=df_all.drop('cat_var_35',axis=1)\n",
    "df_all=df_all.drop('cat_var_36',axis=1)\n",
    "df_all=df_all.drop('cat_var_37',axis=1)\n",
    "df_all=df_all.drop('cat_var_38',axis=1)\n",
    "df_all=df_all.drop('cat_var_39',axis=1)\n",
    "df_all=df_all.drop('cat_var_40',axis=1)\n",
    "df_all=df_all.drop('cat_var_41',axis=1)\n",
    "df_all=df_all.drop('cat_var_42',axis=1)\n",
    "\n",
    "\n",
    "#num_var_3 \n",
    "#cat_var_4 \n",
    "#cat_var_7 \n",
    "#cat_var_9\n",
    "#cat_var_10\n",
    "#1. feature 6 (0.168841)\n",
    "#2. feature 9 (0.164323)\n",
    "#3. feature 0 (0.135010)\n",
    "#4. feature 5 (0.095527)\n",
    "#5. feature 21 (0.041294)\n",
    "#6. feature 23 (0.037992)\n",
    "#7. feature 1 (0.037941)\n",
    "#8. feature 20 (0.031304)\n",
    "#9. feature 7 (0.026507)\n",
    "#10. feature 14 (0.026296)\n",
    "#11. feature 4 (0.025587)\n",
    "#12. feature 12 (0.025329)\n",
    "#13. feature 16 (0.024375)\n",
    "#14. feature 15 (0.021755)\n",
    "#15. feature 22 (0.021519)\n",
    "#16. feature 8 (0.019772)\n",
    "#17. feature 19 (0.017827)\n",
    "#18. feature 18 (0.013268)\n",
    "#19. feature 17 (0.013215)\n",
    "#20. feature 24 (0.013082)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=df_all.drop('num_var_1',axis=1)#good removing trying\n",
    "#df_all=df_all.drop('num_var_2',axis=1)#trying not good by removing \n",
    "#df_all=df_all.drop('num_var_3',axis=1)#******#not good removing\n",
    "#df_all=df_all.drop('num_var_4',axis=1)# not good removing\n",
    "#df_all=df_all.drop('num_var_5',axis=1)#not goodtrying\n",
    "#df_all=df_all.drop('num_var_6',axis=1)#*****#not good to remove\n",
    "#df_all=df_all.drop('num_var_7',axis=1)\n",
    "df_all=df_all.drop('cat_var_1',axis=1)#*******\n",
    "df_all=df_all.drop('cat_var_2',axis=1)#eighth ok\n",
    "df_all=df_all.drop('cat_var_3',axis=1)#******\n",
    "df_all=df_all.drop('cat_var_4',axis=1)# not good \n",
    "df_all=df_all.drop('cat_var_5',axis=1) #not good \n",
    "#df_all=df_all.drop('cat_var_6',axis=1)#******\n",
    "df_all=df_all.drop('cat_var_6_new',axis=1)\n",
    "df_all=df_all.drop('cat_var_7',axis=1) #not good \n",
    "#df_all=df_all.drop('cat_var_8',axis=1)\n",
    "df_all=df_all.drop('cat_var_9',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_10',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_11',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_12',axis=1) #not good\n",
    "df_all=df_all.drop('cat_var_13',axis=1)#not good\n",
    "#df_all=df_all.drop('cat_var_14',axis=1)\n",
    "#df_all=df_all.drop('cat_var_15',axis=1)\n",
    "df_all=df_all.drop('cat_var_16',axis=1)#not good\n",
    "#df_all=df_all.drop('cat_var_17',axis=1)\n",
    "df_all=df_all.drop('cat_var_18',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_19',axis=1) #not good \n",
    "df_all=df_all.drop('cat_var_20',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_21',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_22',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_23',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_24',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_25',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_26',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_27',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_28',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_29',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_30',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_31',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_32',axis=1)#not good new one \n",
    "df_all=df_all.drop('cat_var_33',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_34',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_35',axis=1) #not good new one\n",
    "df_all=df_all.drop('cat_var_36',axis=1)#not good new one\n",
    "df_all=df_all.drop('cat_var_37',axis=1)#trail\n",
    "df_all=df_all.drop('cat_var_38',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_39',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_40',axis=1)#not good\n",
    "df_all=df_all.drop('cat_var_41',axis=1) #not good\n",
    "#df_all=df_all.drop('cat_var_42',axis=1) good result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all=df_all.drop('cat_var_1',axis=1)#*******\n",
    "\n",
    "df_all=df_all.drop('cat_var_3',axis=1)#******\n",
    "\n",
    "df_all=df_all.drop('cat_var_6',axis=1)#******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.apply(lambda x:x.fillna(x.value_counts().index[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 872444 entries, 0 to 523465\n",
      "Data columns (total 49 columns):\n",
      "num_var_1     872444 non-null float64\n",
      "num_var_2     872444 non-null float64\n",
      "num_var_3     872444 non-null float64\n",
      "num_var_4     872444 non-null float64\n",
      "num_var_5     872444 non-null float64\n",
      "num_var_6     872444 non-null float64\n",
      "num_var_7     872444 non-null float64\n",
      "cat_var_1     872444 non-null object\n",
      "cat_var_2     872444 non-null object\n",
      "cat_var_3     872444 non-null object\n",
      "cat_var_4     872444 non-null object\n",
      "cat_var_5     872444 non-null object\n",
      "cat_var_6     872444 non-null object\n",
      "cat_var_7     872444 non-null object\n",
      "cat_var_8     872444 non-null object\n",
      "cat_var_9     872444 non-null object\n",
      "cat_var_10    872444 non-null object\n",
      "cat_var_11    872444 non-null object\n",
      "cat_var_12    872444 non-null object\n",
      "cat_var_13    872444 non-null object\n",
      "cat_var_14    872444 non-null object\n",
      "cat_var_15    872444 non-null object\n",
      "cat_var_16    872444 non-null object\n",
      "cat_var_17    872444 non-null object\n",
      "cat_var_18    872444 non-null object\n",
      "cat_var_19    872444 non-null int64\n",
      "cat_var_20    872444 non-null int64\n",
      "cat_var_21    872444 non-null int64\n",
      "cat_var_22    872444 non-null int64\n",
      "cat_var_23    872444 non-null int64\n",
      "cat_var_24    872444 non-null int64\n",
      "cat_var_25    872444 non-null int64\n",
      "cat_var_26    872444 non-null int64\n",
      "cat_var_27    872444 non-null int64\n",
      "cat_var_28    872444 non-null int64\n",
      "cat_var_29    872444 non-null int64\n",
      "cat_var_30    872444 non-null int64\n",
      "cat_var_31    872444 non-null int64\n",
      "cat_var_32    872444 non-null int64\n",
      "cat_var_33    872444 non-null int64\n",
      "cat_var_34    872444 non-null int64\n",
      "cat_var_35    872444 non-null int64\n",
      "cat_var_36    872444 non-null int64\n",
      "cat_var_37    872444 non-null int64\n",
      "cat_var_38    872444 non-null int64\n",
      "cat_var_39    872444 non-null int64\n",
      "cat_var_40    872444 non-null int64\n",
      "cat_var_41    872444 non-null int64\n",
      "cat_var_42    872444 non-null int64\n",
      "dtypes: float64(7), int64(24), object(18)\n",
      "memory usage: 332.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_all.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_var_2        0\n",
       "num_var_3        0\n",
       "num_var_4        0\n",
       "num_var_5        0\n",
       "num_var_6        0\n",
       "num_var_7        0\n",
       "cat_var_6        0\n",
       "cat_var_8        0\n",
       "cat_var_14       0\n",
       "cat_var_15       0\n",
       "cat_var_17       0\n",
       "cat_var_42       0\n",
       "cat_var_3_new    0\n",
       "cat_var_1_new    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=df_all.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cat_var_6', 'cat_var_8', 'cat_var_14', 'cat_var_15', 'cat_var_17',\n",
      "       'cat_var_3_new', 'cat_var_1_new'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in cf:\n",
    "    df_all[i]=le.fit_transform(df_all[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['num_var_2', 'num_var_3', 'num_var_4', 'num_var_5', 'num_var_6',\n",
      "       'num_var_7', 'cat_var_1', 'cat_var_3', 'cat_var_6', 'cat_var_8',\n",
      "       'cat_var_14', 'cat_var_15', 'cat_var_17', 'cat_var_42'],\n",
      "      dtype='object')\n",
      "num_var_2 2.164597472571879\n",
      "num_var_3 215.24695633421183\n",
      "num_var_4 211.89502505460163\n",
      "num_var_5 249.52685324891908\n",
      "num_var_6 235.3188847422876\n",
      "num_var_7 287.8792501278047\n",
      "cat_var_1 1.5452578774871535\n",
      "cat_var_3 -0.1874920604453583\n",
      "cat_var_6 -0.10980908955388602\n",
      "cat_var_8 1.8698684373215968\n",
      "cat_var_14 2.018433808049143\n",
      "cat_var_15 0.7559582584388439\n",
      "cat_var_17 -1.0197663351355037\n",
      "cat_var_42 0.0\n"
     ]
    }
   ],
   "source": [
    "numeric = df_all._get_numeric_data().columns\n",
    "#numeric=['num_var_1','num_var_5','num_var_6','num_var_7']\n",
    "print(numeric)\n",
    "from scipy.stats import skew\n",
    "for column in numeric:\n",
    "  skness = skew(df_all[column])\n",
    "  print(column,skness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import boxcox1p\n",
    "from scipy.special import boxcox\n",
    "num=[  'num_var_2', 'num_var_3', 'num_var_4', 'num_var_5', 'num_var_6','num_var_7']\n",
    "lam = -1\n",
    "for column in num:\n",
    "    df_all[column] =   boxcox1p(df_all[column], lam)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#df_all=scaler.fit_transform(df_all)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_all=scaler.fit_transform(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train=len(train)\n",
    "X_train = df_all[:num_train]\n",
    "X_test = df_all[num_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           num_var_2      num_var_3     num_var_4     num_var_5     num_var_6  \\\n",
      "count  348978.000000  348978.000000  3.489780e+05  3.489780e+05  3.489780e+05   \n",
      "mean        0.160586       0.000011  4.604324e-05  8.187931e-06  1.482768e-05   \n",
      "std         0.131499       0.002538  1.999947e-03  7.213736e-04  1.492990e-03   \n",
      "min         0.000317       0.000000  4.000000e-08  0.000000e+00  0.000000e+00   \n",
      "25%         0.084514       0.000000  3.550000e-07  4.671053e-08  4.407895e-08   \n",
      "50%         0.101512       0.000000  1.875000e-06  2.598684e-07  9.868421e-08   \n",
      "75%         0.160833       0.000000  2.105000e-06  2.769737e-07  4.618421e-07   \n",
      "max         1.000000       0.758621  3.750000e-01  2.171053e-01  4.605263e-01   \n",
      "\n",
      "          num_var_7  cat_var_42  \n",
      "count  3.489780e+05      348978  \n",
      "mean   1.942554e-05           0  \n",
      "std    1.462171e-03           0  \n",
      "min    0.000000e+00           0  \n",
      "25%    1.720602e-08           0  \n",
      "50%    8.252516e-08           0  \n",
      "75%    3.571842e-07           0  \n",
      "max    3.542030e-01           0  \n"
     ]
    }
   ],
   "source": [
    "X_train=pd.DataFrame(X_train)\n",
    "print(X_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "num=[  'num_var_2', 'num_var_3', 'num_var_4', 'num_var_5', 'num_var_6','num_var_7']\n",
    "elements = numpy.array(X_train)\n",
    "for num in X_train:\n",
    " mean = numpy.mean(X_train, axis=0)\n",
    " sd = numpy.std(X_train, axis=0)\n",
    " X_train[num] = [x for x in arr if (x > mean - 2 * sd)]\n",
    " X_train[num]= [x for x in final_list if (x < mean + 2 * sd)]\n",
    "print(final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_tr1,X_te1,y_tr1,y_te1=train_test_split(X_train,target,test_size=0.1,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_te1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.701303\tvalid-auc:0.699184\n",
      "[20]\ttrain-auc:0.739268\tvalid-auc:0.741856\n",
      "[40]\ttrain-auc:0.742078\tvalid-auc:0.746328\n",
      "[60]\ttrain-auc:0.744257\tvalid-auc:0.749378\n",
      "[80]\ttrain-auc:0.745773\tvalid-auc:0.751657\n",
      "[100]\ttrain-auc:0.746729\tvalid-auc:0.752099\n",
      "[120]\ttrain-auc:0.747627\tvalid-auc:0.753273\n",
      "[140]\ttrain-auc:0.74822\tvalid-auc:0.753989\n",
      "[160]\ttrain-auc:0.749118\tvalid-auc:0.754906\n",
      "[180]\ttrain-auc:0.750095\tvalid-auc:0.755939\n",
      "[200]\ttrain-auc:0.751135\tvalid-auc:0.75702\n",
      "[220]\ttrain-auc:0.751632\tvalid-auc:0.757636\n",
      "[240]\ttrain-auc:0.752597\tvalid-auc:0.758773\n",
      "[260]\ttrain-auc:0.753864\tvalid-auc:0.760221\n",
      "[280]\ttrain-auc:0.755011\tvalid-auc:0.76126\n",
      "[300]\ttrain-auc:0.756081\tvalid-auc:0.762606\n",
      "[320]\ttrain-auc:0.75719\tvalid-auc:0.763375\n",
      "[340]\ttrain-auc:0.758766\tvalid-auc:0.765033\n",
      "[360]\ttrain-auc:0.760499\tvalid-auc:0.766894\n",
      "[380]\ttrain-auc:0.762334\tvalid-auc:0.76865\n",
      "[400]\ttrain-auc:0.763921\tvalid-auc:0.770565\n",
      "[420]\ttrain-auc:0.76577\tvalid-auc:0.772043\n",
      "[440]\ttrain-auc:0.766913\tvalid-auc:0.773286\n",
      "[460]\ttrain-auc:0.768865\tvalid-auc:0.775379\n",
      "[480]\ttrain-auc:0.770826\tvalid-auc:0.777057\n",
      "[500]\ttrain-auc:0.772523\tvalid-auc:0.778714\n",
      "[520]\ttrain-auc:0.774646\tvalid-auc:0.780917\n",
      "[540]\ttrain-auc:0.776833\tvalid-auc:0.783108\n",
      "[560]\ttrain-auc:0.779157\tvalid-auc:0.785577\n",
      "[580]\ttrain-auc:0.781611\tvalid-auc:0.788292\n",
      "[600]\ttrain-auc:0.783511\tvalid-auc:0.790136\n",
      "[620]\ttrain-auc:0.785807\tvalid-auc:0.792715\n",
      "[640]\ttrain-auc:0.788101\tvalid-auc:0.795087\n",
      "[660]\ttrain-auc:0.790279\tvalid-auc:0.797545\n",
      "[680]\ttrain-auc:0.793432\tvalid-auc:0.800369\n",
      "[700]\ttrain-auc:0.796453\tvalid-auc:0.803046\n",
      "[720]\ttrain-auc:0.798951\tvalid-auc:0.805456\n",
      "[740]\ttrain-auc:0.801413\tvalid-auc:0.807793\n",
      "[760]\ttrain-auc:0.804013\tvalid-auc:0.81064\n",
      "[780]\ttrain-auc:0.80659\tvalid-auc:0.813569\n",
      "[800]\ttrain-auc:0.809423\tvalid-auc:0.816278\n",
      "[820]\ttrain-auc:0.812164\tvalid-auc:0.819125\n",
      "[840]\ttrain-auc:0.814701\tvalid-auc:0.821682\n",
      "[860]\ttrain-auc:0.817528\tvalid-auc:0.824694\n",
      "[880]\ttrain-auc:0.818646\tvalid-auc:0.825888\n",
      "[900]\ttrain-auc:0.820984\tvalid-auc:0.828238\n",
      "[920]\ttrain-auc:0.823443\tvalid-auc:0.830765\n",
      "[940]\ttrain-auc:0.826468\tvalid-auc:0.833697\n",
      "[960]\ttrain-auc:0.828799\tvalid-auc:0.83604\n",
      "[980]\ttrain-auc:0.832085\tvalid-auc:0.839125\n",
      "[1000]\ttrain-auc:0.834708\tvalid-auc:0.841502\n",
      "[1020]\ttrain-auc:0.836781\tvalid-auc:0.843521\n",
      "[1040]\ttrain-auc:0.839404\tvalid-auc:0.845962\n",
      "[1060]\ttrain-auc:0.84211\tvalid-auc:0.848766\n",
      "[1080]\ttrain-auc:0.845281\tvalid-auc:0.851834\n",
      "accuracy on train and validation is 0.8479548180535657,0.8544193816764278\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "#'lambda':2\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def multAcc(pred1, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    acc = roc_auc_score(label, pred)\n",
    "    return 'maccuracy', acc\n",
    "#'min_child_weight' :2,\n",
    "#'lambda':2,\n",
    "import xgboost as xgb\n",
    "params = {\n",
    "    'n_trees': 1500, \n",
    "    'eta': 0.001,\n",
    "    'max_depth':30,    \n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7, \n",
    "    'objective': 'binary:logistic',    \n",
    "    'eval_metric': 'auc',\n",
    "    'silent': 0\n",
    "}\n",
    "\n",
    "#dtrain = xgb.DMatrix(data=X_tr1, label=y_tr1)\n",
    "dvalid = xgb.DMatrix(data=X_te1, label=y_te1)\n",
    "#watchlist = [(dtr, 'train'),(dte, 'eval')]\n",
    "\n",
    "dtrain = xgb.DMatrix(data=X_train, label=target)\n",
    "#dvalid = xgb.DMatrix(data=X_train, label=target)\n",
    "#dtrain = xgb.DMatrix(data=X_tr1, label=y_tr1)\n",
    "#dvalid = xgb.DMatrix(data=X_te1, label=y_te1)\n",
    "dtest = xgb.DMatrix(data=X_test)\n",
    "#watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "watchlist = [(dtrain, 'train'),(dvalid, 'valid')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_boost_rounds = 10\n",
    "#model = xgb.train(params, dtrain, num_boost_round=num_boost_rounds)\n",
    "#model=xgb.train(params, dtrain, 480, watchlist, maximize=True,verbose_eval=20)\n",
    "model=xgb.train(params, dtrain, 1100, watchlist, maximize=True,verbose_eval=20)\n",
    "\n",
    "#model=xgb.train(params, dtrain, 1000, watchlist, maximize=True, verbose_eval=20, early_stopping_rounds=60, feval=multAcc)\n",
    "\n",
    "#pred = model.predict(dvalid)\n",
    "prediction1=list()\n",
    "prediction2=list()\n",
    "\n",
    "pred1 = model.predict(dtrain)\n",
    "pred2 = model.predict(dvalid)\n",
    "\n",
    "\n",
    "acc1=roc_auc_score(target,pred1)        \n",
    "#acc1=roc_auc_score(y_tr1,pred1)\n",
    "acc2=roc_auc_score(y_te1,pred2)\n",
    "\n",
    "print(\"accuracy on train and validation is {},{}\".format(acc1,acc2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.73350 , total 1.70038\n",
    "### 0.73315 ,total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy on train and validation is 0.9559857361181865,0.9348386727033068\n",
    "accuracy on train and validation is 0.9609494396332144,0.9337497850879707\n",
    "accuracy on train and validation is 0.9576082985292748,0.9335921829331194\n",
    "\n",
    "accuracy on train and validation is 0.9646868887055933,0.7198817491541584\n",
    "train-auc:0.80738\tvalid-auc:0.725456\n",
    "accuracy on train and validation is 0.7607289651756205,0.7307310281357506-'eta': 0.05,'max_depth':5,'min_child_weight' :2\n",
    "accuracy on train and validation is 0.8245539797958347,0.7326687941225676-'eta': 0.001,'max_depth':25,'min_child_weight' :1\n",
    "\n",
    "[560]\ttrain-auc:0.754034\tvalid-auc:0.741318 without Min Max, eta-0.0006,max_depth-26,gamma-2,'min_child_weight' :4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/lightgbm/engine.py:99: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/usr/local/lib/python3.5/dist-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n",
      "r2 score train and evaluation of light gbm is 0.8235866544249746,0.7367599868017238\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "model_lgb = lgb.LGBMRegressor(objective='binary',metric='auc',num_leaves=1588,\n",
    "                              learning_rate=0.005, n_estimators=9720,\n",
    "                              max_bin = 185, bagging_fraction = 1,max_depth=90,                    \n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =3, num_iterations=1400,min_sum_hessian_in_leaf = 11)\n",
    "model_lgb.fit(X_tr1,y_tr1)\n",
    "#model_lgb.fit(X_train,target)\n",
    "lgb_pred = model_lgb.predict(X_tr1)\n",
    "lgb_pred1 = model_lgb.predict(X_te1)\n",
    "\n",
    "r2=roc_auc_score(y_tr1,lgb_pred)\n",
    "\n",
    "r2_1=roc_auc_score(y_te1,lgb_pred1)\n",
    "print('Training complete')\n",
    "print(\"r2 score train and evaluation of light gbm is {},{}\".format(r2,r2_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "box cox\n",
    "-1---r2 score train and evaluation of light gbm is 0.8265932292608715,0.7394398575735801\n",
    "-2----r2 score train and evaluation of light gbm is 0.8265932292608715,0.7394398575735801\n",
    "+1----r2 score train and evaluation of light gbm is 0.8266251078148508,0.7393552625615953\n",
    "-29---r2 score train and evaluation of light gbm is 0.8239510244145765,0.7399238160106968 with scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 score train and evaluation of light gbm is 0.7211955801467885,0.7105537774568577 only numerical\n",
    "Training complete\n",
    "r2 score train and evaluation of light gbm is 0.7226048157459352,0.713254688770735 with one cat\n",
    "r2 score train and evaluation of light gbm is 0.8229950171532778,0.7293958144081356 with two cat\n",
    "r2 score train and evaluation of light gbm is 0.811202571042417,0.7322010930433891 with three cat\n",
    "r2 score train and evaluation of light gbm is 0.8106595120752037,0.7338130123870836 with four cat\n",
    "r2 score train and evaluation of light gbm is 0.7910648778795026,0.7351943298141538 with five cats\n",
    "r2 score train and evaluation of light gbm is 0.8550678942019903,0.7377007730202509 with six cats\n",
    "r2 score train and evaluation of light gbm is 0.8519351498821768,0.738421466772067 with seven cats\n",
    "r2 score train and evaluation of light gbm is 0.8540896393033813,0.7358492789058676 with eight cats ---cat_var_10\n",
    "r2 score train and evaluation of light gbm is 0.8504362089691919,0.7363463784838145 with eight cats ----cat_var_9\n",
    "r2 score train and evaluation of light gbm is 0.8436705861145699,0.7366235024629035 with eight cats ----cat var_16\n",
    "r2 score train and evaluation of light gbm is 0.8445411691285601,0.7383393995684633 with eight cats -----cat var_2\n",
    "r2 score train and evaluation of light gbm is 0.8446341785139119,0.7370792697767569 with eight cats -----cat var_\n",
    "r2 score train and evaluation of light gbm is 0.8440391909850976,0.736352187248964 with eight cats ------cat_var 19\n",
    "r2 score train and evaluation of light gbm is 0.8488327176192908,0.73569662784735------cat_var 11\n",
    "r2 score train and evaluation of light gbm is 0.8488327176192908,0.73569662784735 eight cats ----cat var\n",
    "r2 score train and evaluation of light gbm is 0.8454858031848306,0.7379739494629641--eight cats--cat var 18\n",
    "r2 score train and evaluation of light gbm is 0.8438024880158487,0.7364847759926767---eight cars ---\n",
    "r2 score train and evaluation of light gbm is 0.8446495197085135,0.7368315185847806---eight cars--cat var 5 \n",
    "missed\n",
    "r2 score train and evaluation of light gbm is 0.8438024880158487,0.7364847759926767--cat var20\n",
    "r2 score train and evaluation of light gbm is 0.8428731567014865,0.7371262333400006---cat var7\n",
    "r2 score train and evaluation of light gbm is 0.8433485168418786,0.7365244591215079---cat var4\n",
    "r2 score train and evaluation of light gbm is 0.8434824421704926,0.7367766426350291 ---cat var 21\n",
    "r2 score train and evaluation of light gbm is 0.8434934409119639,0.737212680923881--cat var 26\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225---cat var 29\n",
    "r2 score train and evaluation of light gbm is 0.8433785546452987,0.7366957008255675----cat var 22\n",
    "r2 score train and evaluation of light gbm is 0.842949271371645,0.736354148031835--cat var 23\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225---cat var 28\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225---cat var 25\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225--cat var 27\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225--cat var 32\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225-- cat var 31\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225--cat var 30\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225\n",
    "r2 score train and evaluation of light gbm is 0.8433603692755152,0.7371021239347225\n",
    "\n",
    "r2 score train and evaluation of light gbm is 0.8214219137063883,0.7376282240540272 adding cat var 37\n",
    "r2 score train and evaluation of light gbm is 0.8214219137063883,0.7376282240540272 adding cat var 38\n",
    "r2 score train and evaluation of light gbm is 0.8214219137063883,0.7376282240540272 adding cat_var_39\n",
    "r2 score train and evaluation of light gbm is 0.8214219137063883,0.7376282240540272 adding cat_var_40\n",
    "r2 score train and evaluation of light gbm is 0.8214219137063883,0.7376282240540272 by adding cat_var_41\n",
    "r2 score train and evaluation of light gbm is 0.8266404052072797,0.7393728966221165 by adding last cat_42\n",
    "\n",
    "\n",
    "r2 score train and evaluation of light gbm is 0.8340119325010424,0.7384660107379051---by removing num_var1\n",
    "r2 score train and evaluation of light gbm is 0.7743071638064267,0.7355860751866304--by removing num_var2\n",
    "r2 score train and evaluation of light gbm is 0.7889294721168967,0.7330956645186211--by removing num_var3\n",
    "r2 score train and evaluation of light gbm is 0.7859413313221267,0.7319031016598395--by removing num_var4\n",
    "r2 score train and evaluation of light gbm is 0.7834585627301098,0.7309474212826836---by removing num_var 5\n",
    "r2 score train and evaluation of light gbm is 0.76817167514278,0.7282505873583524---by removing num_var 6\n",
    "r2 score train and evaluation of light gbm is 0.7469204753465812,0.7120390207045253-- by removing num_var7\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_pred = model_lgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "r2 score train and evaluation of lr is 0.5843703604107657,0.5874961395734591\n",
      "training complete\n",
      "r2 score train and evaluation of extra tree  is 0.9377531287244508,0.6956478151749618\n",
      "training complete\n",
      "r2 score train and evaluation of ada boost is 0.6233143836147523,0.6297130283893225\n",
      "training complete\n",
      "r2 score train and evaluation of random forest is 0.8629417257455976,0.6971573755929799\n",
      "training complete\n",
      "r2 score train and evaluation of knn is 0.7005005036468632,0.6857248774045399\n",
      "training complete\n",
      "r2 score train and evaluation of gradient boost is 0.6540748696556314,0.6561600850659461\n",
      "training complete\n",
      "r2 score train and evaluation of xgb is 0.651514350882824,0.6524114231487601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "r2 score train and evaluation of sgd is 0.5861780768753503,0.5877661908832079\n",
      "training complete\n",
      "r2 score train and evaluation of tree is 0.9301173043637593,0.6992090123986837\n",
      "training complete\n",
      "r2 score train and evaluation of naive bayes is 0.5920790971161409,0.5946087950331336\n",
      "training complete\n",
      "r2 score train and evaluation of BR is 0.8859892323083691,0.70129897320337\n",
      "training complete\n",
      "r2 score train and evaluation of voting classifier is 0.9331987456403035,0.7055153705682683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "#from sklearn.linear_model import \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.linear_model import ARDRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "lr = LogisticRegression()\n",
    "svc = SVC(probability=True)\n",
    "#ridge = Ridge(random_state=1)\n",
    "#svr= SVR()\n",
    "#Elasticnet=ElasticNet()\n",
    "extra_tree=ExtraTreesClassifier()\n",
    "#lasso=Lasso()\n",
    "ada_boost=AdaBoostClassifier()\n",
    "#Bayesianridge=BayesianRidge()\n",
    "random=RandomForestClassifier()\n",
    "knn=KNeighborsClassifier()\n",
    "gradient_boost=GradientBoostingClassifier()\n",
    "xgb = XGBClassifier() \n",
    "#huber=HuberRegressor()\n",
    "sgd=SGDClassifier(loss=\"log\")\n",
    "tree=DecisionTreeClassifier()\n",
    "naive_bayes=GaussianNB()\n",
    "BR=BaggingClassifier()\n",
    "#ARD=ARDRegression()\n",
    "\n",
    "lr.fit(X_tr1, y_tr1)\n",
    "lr_pred=lr.predict(X_tr1)\n",
    "lr_pred1=lr.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,lr_pred)\n",
    "r2_1=roc_auc_score(y_te1,lr_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of lr is {},{}\".format(r2,r2_1))\n",
    "\n",
    "#svc.fit(X_tr1, y_tr1)\n",
    "#svc_pred=svc.predict(X_tr1)\n",
    "#svc_pred1=svc.predict(X_te1)\n",
    "#r2=roc_auc_score(y_tr1,svc_pred)\n",
    "#r2_1=roc_auc_score(y_te1,svc_pred1)\n",
    "#print('training complete')\n",
    "#print(\"r2 score train and evaluation of svc  is {},{}\".format(r2,r2_1))\n",
    "\n",
    "extra_tree.fit(X_tr1, y_tr1)\n",
    "extra_tree_pred=extra_tree.predict(X_tr1)\n",
    "extra_tree_pred1=extra_tree.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,extra_tree_pred)\n",
    "r2_1=roc_auc_score(y_te1,extra_tree_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of extra tree  is {},{}\".format(r2,r2_1))\n",
    "\n",
    "ada_boost.fit(X_tr1, y_tr1)\n",
    "ada_boost_pred=ada_boost.predict(X_tr1)\n",
    "ada_boost_pred1=ada_boost.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,ada_boost_pred)\n",
    "r2_1=roc_auc_score(y_te1,ada_boost_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of ada boost is {},{}\".format(r2,r2_1))\n",
    "\n",
    "random.fit(X_tr1, y_tr1)\n",
    "random_pred=random.predict(X_tr1)\n",
    "random_pred1=random.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,random_pred)\n",
    "r2_1=roc_auc_score(y_te1,random_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of random forest is {},{}\".format(r2,r2_1))\n",
    "\n",
    "knn.fit(X_tr1, y_tr1)\n",
    "knn_pred=knn.predict(X_tr1)\n",
    "knn_pred1=knn.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,knn_pred)\n",
    "r2_1=roc_auc_score(y_te1,knn_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of knn is {},{}\".format(r2,r2_1))\n",
    "\n",
    "gradient_boost.fit(X_tr1, y_tr1)\n",
    "gradient_boost_pred=gradient_boost.predict(X_tr1)\n",
    "gradient_boost_pred1=gradient_boost.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,gradient_boost_pred)\n",
    "r2_1=roc_auc_score(y_te1,gradient_boost_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of gradient boost is {},{}\".format(r2,r2_1))\n",
    "\n",
    "xgb.fit(X_tr1, y_tr1)\n",
    "xgb_pred=xgb.predict(X_tr1)\n",
    "xgb_pred1=xgb.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,xgb_pred)\n",
    "r2_1=roc_auc_score(y_te1,xgb_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of xgb is {},{}\".format(r2,r2_1))\n",
    "\n",
    "sgd.fit(X_tr1, y_tr1)\n",
    "sgd_pred=sgd.predict(X_tr1)\n",
    "sgd_pred1=sgd.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,sgd_pred)\n",
    "r2_1=roc_auc_score(y_te1,sgd_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of sgd is {},{}\".format(r2,r2_1))\n",
    "\n",
    "tree.fit(X_tr1, y_tr1)\n",
    "tree_pred=tree.predict(X_tr1)\n",
    "tree_pred1=tree.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,tree_pred)\n",
    "r2_1=roc_auc_score(y_te1,tree_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of tree is {},{}\".format(r2,r2_1))\n",
    "\n",
    "naive_bayes.fit(X_tr1, y_tr1)\n",
    "naive_bayes_pred=naive_bayes.predict(X_tr1)\n",
    "naive_bayes_pred1=naive_bayes.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,naive_bayes_pred)\n",
    "r2_1=roc_auc_score(y_te1,naive_bayes_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of naive bayes is {},{}\".format(r2,r2_1))\n",
    "\n",
    "BR.fit(X_tr1, y_tr1)\n",
    "BR_pred=BR.predict(X_tr1)\n",
    "BR_pred1=BR.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,BR_pred)\n",
    "r2_1=roc_auc_score(y_te1,BR_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of BR is {},{}\".format(r2,r2_1))\n",
    "\n",
    "#eclf = VotingClassifier(estimators=[('lr', lr), ('extra_tree', extra_tree),('ada boost', ada_boost),('random forest', random)\n",
    "#,('knn',knn),('gradient boost', gradient_boost),('xgb', xgb),('sgd', sgd),('tree', tree),('naive bayes', naive_bayes)\n",
    "#,('Bagging classifier', BR)], voting='soft', weights=[1,5,1,5,1,1,1,1,5,1,5])\n",
    "\n",
    "eclf = VotingClassifier(estimators=[ ('extra_tree', extra_tree),('random forest', random)\n",
    ",('tree', tree),('Bagging classifier', BR)], voting='soft', weights=[1,1,1,1])\n",
    "\n",
    "eclf.fit(X_tr1, y_tr1)\n",
    "eclf_pred=eclf.predict(X_tr1)\n",
    "eclf_pred1=eclf.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,eclf_pred)\n",
    "r2_1=roc_auc_score(y_te1,eclf_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of voting classifier is {},{}\".format(r2,r2_1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=85, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=True)\n",
      "training complete\n",
      "r2 score train and evaluation of BR is 0.8493436364906516,0.7003013372669034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators': [85,115,125]} ]\n",
    "scorer=make_scorer(roc_auc_score)\n",
    "BR=BaggingClassifier(warm_start=True,n_estimators: 85)\n",
    "grid_obj=GridSearchCV(BR,parameters,cv=5,n_jobs=1,scoring=scorer)\n",
    "grid_fit=grid_obj.fit(X_tr1, y_tr1)\n",
    "best_BR=grid_fit.best_estimator_\n",
    "BR_pred=best_BR.predict(X_tr1)\n",
    "BR_pred1=best_BR.predict(X_te1)\n",
    "r2=roc_auc_score(y_tr1,BR_pred)\n",
    "r2_1=roc_auc_score(y_te1,BR_pred1)\n",
    "print(best_BR)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of BR is {},{}\".format(r2,r2_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=25, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "training complete\n",
      "r2 score train and evaluation of BR is 0.845639335732476,0.6996325241850447\n"
     ]
    }
   ],
   "source": [
    "print(best_BR)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation of BR is {},{}\".format(r2,r2_1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "r2 score train and evaluation is 0.9380780945018847,0.6970714127942684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Stacked Classifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from mlxtend.data import boston_housing_data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "#from sklearn.linear_model import \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.linear_model import ARDRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import r2_score\n",
    "lr = LogisticRegression()\n",
    "svc = SVC(probability=True)\n",
    "#ridge = Ridge(random_state=1)\n",
    "#svr= SVR()\n",
    "#Elasticnet=ElasticNet()\n",
    "extra_tree=ExtraTreesClassifier()\n",
    "#lasso=Lasso()\n",
    "ada_boost=AdaBoostClassifier()\n",
    "#Bayesianridge=BayesianRidge()\n",
    "random=RandomForestClassifier()\n",
    "knn=KNeighborsClassifier()\n",
    "gradient_boost=GradientBoostingClassifier()\n",
    "xgb = XGBClassifier() \n",
    "#huber=HuberRegressor()\n",
    "sgd=SGDClassifier(loss=\"log\")\n",
    "tree=DecisionTreeClassifier()\n",
    "naive_bayes=GaussianNB()\n",
    "BR=BaggingClassifier(n_estimators=85)\n",
    "#ARD=ARDRegression()\n",
    "\n",
    "#stregr = StackingClassifier(classifiers=[svc,random,gradient_boost,knn,ada_boost,tree,lr], \n",
    "                           #use_probas=True,meta_classifier= xgb)\n",
    "stregr = StackingClassifier(classifiers=[random,xgb,gradient_boost,knn,ada_boost,tree,naive_bayes,extra_tree,lr], \n",
    "                           meta_classifier= BR)\n",
    "\n",
    "#params = {'lasso__alpha': [ 1.0, 2.0],\n",
    "         # 'ridge__alpha': [ 1.0, 2.0]\n",
    "         # }\n",
    "\n",
    "#grid = GridSearchCV(estimator=stregr, \n",
    "                   # param_grid=params, \n",
    "                    #cv=5,\n",
    "                    #refit=True)\n",
    "#grid.fit(X_tr1, y_tr1)\n",
    "\n",
    "#for params, mean_score, scores in grid.grid_scores_:\n",
    "        #print(\"%0.3f +/- %0.2f %r\"\n",
    "              #% (mean_score, scores.std() / 2.0, params))\n",
    "\n",
    "\n",
    "\n",
    "#pred= grid.best_estimator_.predict(X_tr1)\n",
    "#pred1=grid.best_estimator_.predict(X_te1)\n",
    "\n",
    "#stregr = StackingRegressor(regressors=[ lasso,ridge,Elasticnet,Bayesianridge,gradient_boost,knn], \n",
    "                           #meta_regressor= random_forest)\n",
    "#print(grid.best_estimator_)\n",
    "# Training the stacking classifier\n",
    "\n",
    "#stregr.fit(X_train, target)\n",
    "stregr.fit(X_tr1, y_tr1)\n",
    "stack_pred=stregr.predict(X_tr1)\n",
    "stack_pred1=stregr.predict(X_te1)\n",
    "# Evaluate and visualize the fit\n",
    "#r2=r2_score(y_tr1,pred)\n",
    "r2=roc_auc_score(y_tr1,stack_pred)\n",
    "r2_1=roc_auc_score(y_te1,stack_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation is {},{}\".format(r2,r2_1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.39068469e-01   3.50371076e-02   2.18621802e-05   6.11629591e-03\n",
      "   2.59179679e-02   9.42993477e-02   1.66714111e-01   5.38367740e-02\n",
      "   1.60603938e-02   1.64744909e-01   2.82536455e-03   8.28544451e-03\n",
      "   4.28601910e-02   1.92706786e-03   3.20078351e-02   1.87111240e-02\n",
      "   2.52203538e-02   1.27463345e-02   9.41722201e-03   2.12368976e-02\n",
      "   2.80582417e-02   2.19034515e-02   4.77596349e-03   4.19630327e-02\n",
      "   8.55747010e-03   6.80458668e-03   2.44376015e-03   1.83693574e-03\n",
      "   1.19176540e-03   3.95199833e-04   2.99989121e-03   1.81997251e-04\n",
      "   1.25795547e-03   0.00000000e+00   2.75138194e-05   1.34669604e-04\n",
      "   3.91689570e-04   0.00000000e+00   1.85831579e-06   1.12660955e-05\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   7.67930330e-06   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "Feature ranking:\n",
      "1. feature 6 (0.168841)\n",
      "2. feature 9 (0.164323)\n",
      "3. feature 0 (0.135010)\n",
      "4. feature 5 (0.095527)\n",
      "5. feature 21 (0.041294)\n",
      "6. feature 23 (0.037992)\n",
      "7. feature 1 (0.037941)\n",
      "8. feature 20 (0.031304)\n",
      "9. feature 7 (0.026507)\n",
      "10. feature 14 (0.026296)\n",
      "11. feature 4 (0.025587)\n",
      "12. feature 12 (0.025329)\n",
      "13. feature 16 (0.024375)\n",
      "14. feature 15 (0.021755)\n",
      "15. feature 22 (0.021519)\n",
      "16. feature 8 (0.019772)\n",
      "17. feature 19 (0.017827)\n",
      "18. feature 18 (0.013268)\n",
      "19. feature 17 (0.013215)\n",
      "20. feature 24 (0.013082)\n",
      "21. feature 25 (0.009202)\n",
      "22. feature 11 (0.008804)\n",
      "23. feature 3 (0.006342)\n",
      "24. feature 30 (0.002714)\n",
      "25. feature 26 (0.002544)\n",
      "26. feature 13 (0.002433)\n",
      "27. feature 10 (0.002145)\n",
      "28. feature 27 (0.001949)\n",
      "29. feature 32 (0.001353)\n",
      "30. feature 35 (0.000528)\n",
      "31. feature 28 (0.000462)\n",
      "32. feature 29 (0.000387)\n",
      "33. feature 34 (0.000154)\n",
      "34. feature 31 (0.000149)\n",
      "35. feature 33 (0.000024)\n",
      "36. feature 2 (0.000022)\n",
      "37. feature 38 (0.000011)\n",
      "38. feature 37 (0.000008)\n",
      "39. feature 36 (0.000003)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEICAYAAABMGMOEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff4a48727b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "forest =ExtraTreesClassifier()\n",
    "forest.fit(X_train,target)\n",
    "\n",
    "print(clf.feature_importances_ )\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train-maccuracy:0.94669\teval-maccuracy:0.933016\n",
    "train-maccuracy:0.944466\teval-maccuracy:0.933028\n",
    "train-maccuracy:0.946354\teval-maccuracy:0.934277\n",
    "train-maccuracy:0.94012\teval-maccuracy:0.934438\n",
    "train-maccuracy:0.946963\teval-maccuracy:0.934294\n",
    "accuracy on train and validation is 0.9867358634742741,0.9311708407358588\n",
    "accuracy on train and validation is 0.9379807692307692,0.9341796091466562\n",
    "accuracy on train and validation is 0.93962366276108,0.9342942288956387\n",
    "accuracy on train and validation is 0.9481278655119715,0.9345521233308499\n",
    "accuracy on train and validation is 0.9636462047885889,0.9338930597741991\n",
    "accuracy on train and validation is 0.9529323739174732,0.9344088486446215 #removing all\n",
    "accuracy on train and validation is 0.9586920529801325,0.9356696658834317 #removing one\n",
    "accuracy on train and validation is 0.9587939378502293,0.9334918906527595 #without removing one\n",
    "accuracy on train and validation is 0.9530788334182374,0.9348100177660611#without top 2\n",
    "accuracy on train and validation is 0.9392543301069791,0.9327468622843716 #without top 1 and 61 depth,0.01\n",
    "accuracy on train and validation is 0.9536837748344371,0.9333772709037768#without top 1 and 61 depth, 0.02\n",
    "accuracy on train and validation is 0.9375413907284769,0.932431657974669--0.04,depth 10\n",
    "accuracy on train and validation is 0.9383628374936321,0.932804172158863--0.05,depth 10\n",
    "accuracy on train and validation is 0.9397510188487009,0.9329187919078458--0.05,depth 11\n",
    "\n",
    "0.73\n",
    "accuracy on train and validation is 0.993054003404226,0.9927216459395954\n",
    "accuracy on train and validation is 0.9451222618441162,0.9362714195655911 #removing 4\n",
    "accuracy on train and validation is 0.9452336984207845,0.9340363344604276\n",
    "\n",
    "accuracy on train and validation is 0.9510857106469689,0.9348386727033068\n",
    "\n",
    "accuracy on train and validation is 0.9439760570555272,0.9353831165109748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submissions_1.csv')\n",
    "sub['target'] = pred\n",
    "#sub['target'] = sub['target'].astype(int)\n",
    "sub.to_csv('jishnu.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "out=np_utils.to_categorical(target,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3148, activation=\"relu\", kernel_initializer=\"he_normal\", input_dim=18)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "import keras.backend as K\n",
    "def auc_obj(y_true, y_pred):\n",
    "    auc=roc_auc_score(y_true, y_pred)\n",
    "    return k.auc\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(18,)))\n",
    "model.add(Dense(3148, activation='relu', init='he_normal',input_dim=18))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(28, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(22, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282672 samples, validate on 31408 samples\n",
      "Epoch 1/10\n",
      "282672/282672 [==============================] - 118s 418us/step - loss: 0.3035 - acc: 0.9070 - val_loss: 0.2839 - val_acc: 0.9141\n",
      "Epoch 2/10\n",
      "282672/282672 [==============================] - 115s 406us/step - loss: 0.2875 - acc: 0.9131 - val_loss: 0.2834 - val_acc: 0.9148\n",
      "Epoch 3/10\n",
      "282672/282672 [==============================] - 106s 376us/step - loss: 0.2861 - acc: 0.9135 - val_loss: 0.2825 - val_acc: 0.9146\n",
      "Epoch 4/10\n",
      "282672/282672 [==============================] - 106s 375us/step - loss: 0.2851 - acc: 0.9137 - val_loss: 0.2826 - val_acc: 0.9149\n",
      "Epoch 5/10\n",
      "282672/282672 [==============================] - 107s 380us/step - loss: 0.2844 - acc: 0.9139 - val_loss: 0.2812 - val_acc: 0.9151\n",
      "Epoch 6/10\n",
      "282672/282672 [==============================] - 105s 371us/step - loss: 0.2839 - acc: 0.9141 - val_loss: 0.2804 - val_acc: 0.9153\n",
      "Epoch 7/10\n",
      "282672/282672 [==============================] - 104s 367us/step - loss: 0.2835 - acc: 0.9143 - val_loss: 0.2807 - val_acc: 0.9153\n",
      "Epoch 8/10\n",
      "282672/282672 [==============================] - 104s 368us/step - loss: 0.2833 - acc: 0.9143 - val_loss: 0.2807 - val_acc: 0.9154\n",
      "Epoch 9/10\n",
      "282672/282672 [==============================] - 104s 369us/step - loss: 0.2831 - acc: 0.9144 - val_loss: 0.2796 - val_acc: 0.9154\n",
      "Epoch 10/10\n",
      "282672/282672 [==============================] - 104s 366us/step - loss: 0.2826 - acc: 0.9147 - val_loss: 0.2803 - val_acc: 0.9153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce81f3aa90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_tr1,y_tr1,  validation_split=0.1, shuffle=True, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete\n",
      "r2 score train and evaluation is 0.8994023047871758,0.691094336293827\n"
     ]
    }
   ],
   "source": [
    "dnn_pred=stregr.predict(X_tr1)\n",
    "dnn_pred1=stregr.predict(X_te1)\n",
    "# Evaluate and visualize the fit\n",
    "#r2=r2_score(y_tr1,pred)\n",
    "r2=roc_auc_score(y_tr1,dnn_pred)\n",
    "r2_1=roc_auc_score(y_te1,dnn_pred1)\n",
    "print('training complete')\n",
    "print(\"r2 score train and evaluation is {},{}\".format(r2,r2_1))#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cd5fd7337a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#pred = model.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#pred = model.predict(X_test)\n",
    "pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred=pd.read_csv('krishna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgb_pred['target'] *0.9 + lgb_pred*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submissions_1.csv')\n",
    "sub['target'] = pred\n",
    "#sub['target'] = sub['target'].astype(int)\n",
    "sub.to_csv('kesava.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
